{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: \"The main reason for why we do data exploration is; we want to find relations between the data that we are given to find some meaningful results.\"\n",
    "\n",
    "Q2.1: \"There would be null values from people who disregarded this review. Another unwanted data would be non-numeric answers. People can also write down numbers out of (1,5).\"\n",
    "\n",
    "Q2.2: \"I would have calculated the mean of the given answers after cleaning the data. Then I'd consider 3, as being the mean of 1&5, as a neutral value. If the mean of the data would be below 3, then I'd say the customers are not satisfied with the company so far. For the option that if mean would be greater than 3, then I'd say the customers are satisfied.\"\n",
    "\n",
    "Q2.3: \"I can start by the satisfaction of the different genders. That's easy to do since I make only one arrangement. Then I can divide the customers into their age groups, such as 17-24, 25-32, etc. Then I'll try to see what they are telling me.\"\n",
    "\n",
    "Q3: \"They should be taken care of, since I'd have to get all of the information that I could get. I'll use mean of the missing values if the missing data is numerical. For example, for the age groups, I'd use the mean of the 17-24 for a teenage girl's missing data. That's because, the Central Limit Theorem works.\"\n",
    "\n",
    "Q4: \"Outliers are important as long as I consider them. Unfortunately, for a company with a large dataset, a few people may not be so crucial.\"\n",
    "\n",
    "Q5: \"I start by reading the description, if there's any. After that, I try to understand any columns or rows or that kind of thing that I do not know. Then I check the data, whether they are given in numerical or alphabetical way. Then I try to make a correlation between the data in my mind. I try to find the right question; 'is there any relation between this and that?'. Only then the technical approach comes to life. I'll try to do Exploratory Data Analysis by starting with the Data Cleaning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
